{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN from Scratch\n",
    "\n",
    "\n",
    "Este cuaderno implementa KNN utilizando únicamente numpy y datos del dataset Iris.\n",
    "Incluye pasos para cargar datos, dividirlos, calcular distancias y predecir etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Función para cargar el dataset Iris\n",
    "\n",
    " Usamos la librería sklearn para obtener el dataset Iris, pero la implementación del algoritmo\n",
    "será puramente con NumPy. El dataset contiene 150 muestras de flores con 4 características\n",
    " y 3 clases (setosa, versicolor, virginica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_iris_dataset_csv():\n",
    "  data = np.genfromtxt('Iris.csv', delimiter=',', dtype=str, skip_header=1) \n",
    "  features = data[:, 1:-1].astype(float) \n",
    "  classes = data[:, -1]    \n",
    "  unique_classes, encoded_classes = np.unique(classes, return_inverse=True)\n",
    "  \n",
    "  print(\"Clases únicas:\", unique_classes)\n",
    "  print(\"Clases codificadas:\", encoded_classes[:5]) \\\n",
    "      \n",
    "  print(\"\\nCaracterísticas:\")\n",
    "  print(features[:5])  # Mostrar las primeras 5 filas \n",
    "  print(\"\\nClases codificadas:\")\n",
    "  print(encoded_classes[:5])  # Mostrar las primeras 5 clases codificadas \n",
    "  \n",
    "  return features, encoded_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de Característica y Clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clases únicas: ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n",
      "Clases codificadas: [0 0 0 0 0]\n",
      "\n",
      "Características:\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "\n",
      "Clases codificadas:\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "features, encoded_classes =load_iris_dataset_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2. División de datos en entrenamiento y prueba\n",
    "Para evaluar nuestro modelo, dividimos los datos en dos partes:\n",
    " - Conjunto de entrenamiento (80%)\n",
    " - Conjunto de prueba (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, y, test_size=0.2, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)  # Barajamos los índices aleatoriamente\n",
    "    split_index = int(X.shape[0] * (1 - test_size))  # Índice para dividir\n",
    "    train_indices = indices[:split_index]\n",
    "    test_indices = indices[split_index:]\n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividimos los datos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tamaño del conjunto de entrenamiento: 120\n",
      "Tamaño del conjunto de prueba: 30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, encoded_classes)\n",
    "print(\"\\nTamaño del conjunto de entrenamiento:\", X_train.shape[0])\n",
    "print(\"Tamaño del conjunto de prueba:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Función para calcular distancias euclidianas\n",
    "\n",
    "\n",
    "La distancia euclidiana entre dos puntos x1 y x2 es:\n",
    "d = sqrt(sum((x1 - x2)^2))\n",
    "Implementamos esta fórmula con NumPy, soportando cálculos vectorizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2) ** 2, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo de cálculo de distancias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distancias a la primera instancia de prueba:\n",
      " [0.3        3.22800248 2.96984848 0.2        0.75498344]\n"
     ]
    }
   ],
   "source": [
    "example_distances = euclidean_distance(X_train, X_test[0])\n",
    "print(\"\\nDistancias a la primera instancia de prueba:\\n\", example_distances[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 4. Implementación de KNN\n",
    "\n",
    "En KNN, para cada punto de prueba, calculamos las distancias a todos los puntos\n",
    "de entrenamiento, seleccionamos los k vecinos más cercanos y predecimos la clase\n",
    "por mayoría."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predict(X_train, y_train, X_test, k=3):\n",
    "    predictions = []\n",
    "    for x in X_test:\n",
    "        # 1. Calcular distancias desde el punto de prueba a todos los puntos de entrenamiento\n",
    "        distances = euclidean_distance(X_train, x)\n",
    "        # 2. Obtener los índices de los k vecinos más cercanos\n",
    "        nearest_indices = distances.argsort()[:k]\n",
    "        # 3. Obtener las etiquetas de los k vecinos más cercanos\n",
    "        nearest_labels = y_train[nearest_indices]\n",
    "        # 4. Clasificar por mayoría (la clase con más votos)\n",
    "        predicted_label = np.bincount(nearest_labels.astype(int)).argmax()\n",
    "        predictions.append(predicted_label)\n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Realizamos predicciones en el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicciones (primeras 5): [1 0 1 1 0]\n",
      "Etiquetas reales (primeras 5): [1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "k = 5  # Número de vecinos\n",
    "predictions = knn_predict(X_train, y_train, X_test, k)\n",
    "print(\"\\nPredicciones (primeras 5):\", predictions[:5])\n",
    "print(\"Etiquetas reales (primeras 5):\", y_test[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 5. Evaluación del modelo\n",
    "\n",
    "Calculamos la precisión como el porcentaje de etiquetas correctamente predichas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precisión del modelo KNN (k=5): 96.67%\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(predictions == y_test) * 100\n",
    "print(f\"\\nPrecisión del modelo KNN (k={k}): {accuracy:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "program_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
